{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:53:18.285247Z",
     "iopub.status.busy": "2025-05-15T08:53:18.284747Z",
     "iopub.status.idle": "2025-05-15T08:53:22.471226Z",
     "shell.execute_reply": "2025-05-15T08:53:22.470418Z",
     "shell.execute_reply.started": "2025-05-15T08:53:18.285218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:53:22.472870Z",
     "iopub.status.busy": "2025-05-15T08:53:22.472564Z",
     "iopub.status.idle": "2025-05-15T08:53:22.480842Z",
     "shell.execute_reply": "2025-05-15T08:53:22.479940Z",
     "shell.execute_reply.started": "2025-05-15T08:53:22.472850Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_load, processor_load):\n",
    "        self.dataset = dataset_load\n",
    "        self.processor = processor_load\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.dataset[idx]['question']\n",
    "        answer = self.dataset[idx]['answer']\n",
    "        image_id = self.dataset[idx]['image_id']\n",
    "        image_path = self.dataset[idx]['image_path']\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = question\n",
    "\n",
    "        \"\"\"Model Prediction\"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Image - Image loaded from csv file\n",
    "        Text - the question loaded from csv file\n",
    "        Padding - set to max length of the model - BlipProcessor set's this limit\n",
    "        Truncation - If the question is too long, it cuts off extra tokens to fit max length.\n",
    "        Return Tensors - Return the output as PyTorch tensors (default is list of ints).\n",
    "        Return Attention Mask - Tells the model which tokens are actual input and which are padding\n",
    "\n",
    "        Encoding returns\n",
    "        {pixel_values:, input_ids:, attention_mask}\n",
    "        pixel_values - raw pixel values of imafe\n",
    "        input_ids - input model\n",
    "        attention_mask - Returns a tensor like [1, 1, 1, 0, 0]\n",
    "        \"\"\"\n",
    "        \n",
    "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\", return_attention_mask=True)\n",
    "        \n",
    "        \"\"\"Encoding our Answers\"\"\"\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            str(answer),\n",
    "            max_length= 16,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        # Add labels also to the dictionary\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        # Remove extra dimension from the attention mask\n",
    "        encoding[\"attention_mask\"] = encoding[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for dict_key,dict_value in encoding.items():  \n",
    "            encoding[dict_key] = dict_value.squeeze()\n",
    "            \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:53:22.481660Z",
     "iopub.status.busy": "2025-05-15T08:53:22.481466Z",
     "iopub.status.idle": "2025-05-15T08:53:53.838813Z",
     "shell.execute_reply": "2025-05-15T08:53:53.838164Z",
     "shell.execute_reply.started": "2025-05-15T08:53:22.481622Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 08:53:32.141374: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747299212.330882      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747299212.388615      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df43473a582a4e049eb5a0bc8b6894d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28509cead2054246bc2547903098ba04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10e428b8e764a9b8409fb9035472759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c2a7a5529246f8bed2e5146670223b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83499927b28641508d26780ce33ba86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510a169cda404b369fde824e174bd93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e5a9db69b349d0837edc22c56a7a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79392fb10610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "# cached_dir = os.path.join('BLIP_checkpoints')\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")#,cache_dir=cached_dir)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\",use_fast = True)#,cache_dir=cached_dir)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:38:04.814932Z",
     "iopub.status.busy": "2025-05-15T09:38:04.814655Z",
     "iopub.status.idle": "2025-05-15T09:38:06.196992Z",
     "shell.execute_reply": "2025-05-15T09:38:06.196284Z",
     "shell.execute_reply.started": "2025-05-15T09:38:04.814908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "kagglehub.dataset_download(\"hlgsagar1234567/vr-go\")\n",
    "kagglehub.dataset_download(\"rajan56/datacuration\")\n",
    "\n",
    "train_df = pd.read_csv(\"/kaggle/input/datacuration/data_curation_train.csv\")  \n",
    "val_df = pd.read_csv(\"/kaggle/input/datacuration/data_curation_val.csv\")\n",
    "\n",
    "train_df = train_df.sample(n=512, random_state=42).reset_index(drop=True)\n",
    "val_df = val_df.sample(n=128, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(len(train_df),len(val_df))\n",
    "\n",
    "# Convert back to Hugging Face Datasets\n",
    "training_data = Dataset.from_pandas(train_df)\n",
    "valid_data = Dataset.from_pandas(val_df)\n",
    "\n",
    "training_set = VQADataset(dataset_load=training_data, processor_load=processor)\n",
    "valid_set = VQADataset(dataset_load=valid_data, processor_load=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:18:15.623878Z",
     "iopub.status.busy": "2025-05-15T09:18:15.623427Z",
     "iopub.status.idle": "2025-05-15T09:18:15.628243Z",
     "shell.execute_reply": "2025-05-15T09:18:15.627532Z",
     "shell.execute_reply.started": "2025-05-15T09:18:15.623854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Visualising data returned from Dataset, VQA Dataset '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Visualising data returned from Dataset, VQA Dataset \"\"\"\n",
    "# print(training_data.column_names)\n",
    "# for idx in range(1):\n",
    "#     encoding = training_set[idx]\n",
    "#     print(\"Encoding\",encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:38:07.894449Z",
     "iopub.status.busy": "2025-05-15T09:38:07.893814Z",
     "iopub.status.idle": "2025-05-15T09:38:07.900800Z",
     "shell.execute_reply": "2025-05-15T09:38:07.900017Z",
     "shell.execute_reply.started": "2025-05-15T09:38:07.894419Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\"\"\"\n",
    "The DataLoader + collate_fn work on your processed dataset items (training_set) \n",
    "\n",
    "training_set[idx] calls your __getitem__ method, which:\n",
    "Loads the image,\n",
    "Processes it with the processor to create tensors (input_ids, pixel_values, attention_mask),\n",
    "Encodes labels,\n",
    "Returns a dictionary of tensors\n",
    "\"\"\"\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \n",
    "    # Separate each element in the batch\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    \"\"\"input_ids\"\"\"\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)  # Adjust padding_value as needed\n",
    "\n",
    "    \"\"\"labels\"\"\"\n",
    "    # The special value -100 is the default ignore_index in PyTorch’s CrossEntropyLoss.\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    \"\"\"pixel_values\"\"\"\n",
    "    \"\"\"attention_mask\"\"\"\n",
    "    # stacks all items -> [3, H, W] => [batch_size, 3, H, W]\n",
    "    pixel_values_stacked = torch.stack(pixel_values)  \n",
    "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Return the batch as a dictionary\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"pixel_values\": pixel_values_stacked,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"labels\": labels_padded,\n",
    "    }\n",
    "\n",
    "# Use this collate function in your DataLoader\n",
    "train_dataloader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:18:22.887885Z",
     "iopub.status.busy": "2025-05-15T09:18:22.887363Z",
     "iopub.status.idle": "2025-05-15T09:18:22.890901Z",
     "shell.execute_reply": "2025-05-15T09:18:22.890239Z",
     "shell.execute_reply.started": "2025-05-15T09:18:22.887860Z"
    }
   },
   "outputs": [],
   "source": [
    "# for row in train_dataloader:\n",
    "#     for k,v in row.items():\n",
    "#         print(k)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:53:58.382887Z",
     "iopub.status.busy": "2025-05-15T08:53:58.382682Z",
     "iopub.status.idle": "2025-05-15T08:53:58.386354Z",
     "shell.execute_reply": "2025-05-15T08:53:58.385658Z",
     "shell.execute_reply.started": "2025-05-15T08:53:58.382869Z"
    }
   },
   "outputs": [],
   "source": [
    "# for name, module in model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:38:10.942462Z",
     "iopub.status.busy": "2025-05-15T09:38:10.941735Z",
     "iopub.status.idle": "2025-05-15T09:38:10.946952Z",
     "shell.execute_reply": "2025-05-15T09:38:10.946335Z",
     "shell.execute_reply.started": "2025-05-15T09:38:10.942433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text_decoder.bert.encoder.layer.1.crossattention.self.query', 'text_decoder.bert.encoder.layer.2.crossattention.self.query', 'text_decoder.bert.encoder.layer.3.crossattention.self.query', 'text_decoder.bert.encoder.layer.4.crossattention.self.query', 'text_decoder.bert.encoder.layer.5.crossattention.self.query', 'text_decoder.bert.encoder.layer.6.crossattention.self.query', 'text_decoder.bert.encoder.layer.7.crossattention.self.query', 'text_decoder.bert.encoder.layer.8.crossattention.self.query', 'text_decoder.bert.encoder.layer.9.crossattention.self.query', 'text_decoder.bert.encoder.layer.10.crossattention.self.query', 'text_decoder.bert.encoder.layer.11.crossattention.self.query', 'text_decoder.bert.encoder.layer.1.crossattention.self.value', 'text_decoder.bert.encoder.layer.2.crossattention.self.value', 'text_decoder.bert.encoder.layer.3.crossattention.self.value', 'text_decoder.bert.encoder.layer.4.crossattention.self.value', 'text_decoder.bert.encoder.layer.5.crossattention.self.value', 'text_decoder.bert.encoder.layer.6.crossattention.self.value', 'text_decoder.bert.encoder.layer.7.crossattention.self.value', 'text_decoder.bert.encoder.layer.8.crossattention.self.value', 'text_decoder.bert.encoder.layer.9.crossattention.self.value', 'text_decoder.bert.encoder.layer.10.crossattention.self.value', 'text_decoder.bert.encoder.layer.11.crossattention.self.value']\n"
     ]
    }
   ],
   "source": [
    "text_encoder_target_modules = [\n",
    "    f\"text_decoder.bert.encoder.layer.{i}.crossattention.self.query\" for i in range(1, 12)\n",
    "] + [\n",
    "    f\"text_decoder.bert.encoder.layer.{i}.crossattention.self.value\" for i in range(1, 12)\n",
    "]\n",
    "\n",
    "print(text_encoder_target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:38:11.820827Z",
     "iopub.status.busy": "2025-05-15T09:38:11.820249Z",
     "iopub.status.idle": "2025-05-15T09:38:11.866710Z",
     "shell.execute_reply": "2025-05-15T09:38:11.866165Z",
     "shell.execute_reply.started": "2025-05-15T09:38:11.820800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 270,336 || all params: 384,942,908 || trainable%: 0.0702\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, # rank of the low-rank matrices that LoRA uses to modify certain parts of the model\n",
    "    lora_alpha=32, # scale LoRA weights, effectively controlling how strongly the LoRA layers impact the model’s predictions\n",
    "    lora_dropout=0.05, #\n",
    "    bias=\"none\", # whether or not to add biases to the LoRA-modified layers\n",
    "    target_modules=text_encoder_target_modules      # specifies the exact layers in the model where LoRA should be applied, typically in attention mechanisms\n",
    "                                        # [\"q_proj\", \"k_proj\"] are standard as they affect the query and key projections in attention layers\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagar's Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:38:13.989392Z",
     "iopub.status.busy": "2025-05-15T09:38:13.988838Z",
     "iopub.status.idle": "2025-05-15T09:38:13.995099Z",
     "shell.execute_reply": "2025-05-15T09:38:13.994366Z",
     "shell.execute_reply.started": "2025-05-15T09:38:13.989367Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "def get_latest_checkpoint_from_hf(repo_id, token=None):\n",
    "    \"\"\"\n",
    "    Fetch the latest checkpoint folder name or path from a Hugging Face repo.\n",
    "\n",
    "    repo_id: str, e.g. \"username/modelname\"\n",
    "    token: Optional str, your HF token if private repo\n",
    "\n",
    "    Returns:\n",
    "    latest_checkpoint: str or None\n",
    "    \"\"\"\n",
    "\n",
    "    login(token=token)\n",
    "    api = HfApi()\n",
    "    # List all files in the repo root (or a specific folder if you organize checkpoints)\n",
    "    files = api.list_repo_files(repo_id=repo_id, token=token)\n",
    "\n",
    "    # Filter checkpoint folders/files\n",
    "    checkpoints = [f for f in files if f.startswith('checkpoint')]\n",
    "\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    # Assuming checkpoints have incremental naming like checkpoint1, checkpoint2...\n",
    "    # Sort them based on numeric suffix\n",
    "    checkpoints.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "    return checkpoints[-1] \n",
    "\n",
    "def push_tracking_info_to_hub(tracking_info, repo_id, commit_message=\"Update tracking info\"):\n",
    "    with io.BytesIO() as f:\n",
    "        pickle.dump(tracking_info, f)\n",
    "        f.seek(0)\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=f,\n",
    "            path_in_repo=\"tracking_info.pkl\",\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=commit_message,\n",
    "            token=\"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\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:40:27.321479Z",
     "iopub.status.busy": "2025-05-15T09:40:27.320930Z",
     "iopub.status.idle": "2025-05-15T09:41:40.334087Z",
     "shell.execute_reply": "2025-05-15T09:41:40.333341Z",
     "shell.execute_reply.started": "2025-05-15T09:40:27.321452Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.31.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.4.26)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert_score) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "Successfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:47:26.767356Z",
     "iopub.status.busy": "2025-05-15T09:47:26.767033Z",
     "iopub.status.idle": "2025-05-15T09:47:26.887504Z",
     "shell.execute_reply": "2025-05-15T09:47:26.886920Z",
     "shell.execute_reply.started": "2025-05-15T09:47:26.767323Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from huggingface_hub import HfApi, login\n",
    "from peft import PeftModel\n",
    "from transformers import BlipForQuestionAnswering, BlipProcessor\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "login(token=\"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\")\n",
    "api = HfApi()\n",
    "REPO_ID = \"adityaav80/blip-basevqa-finetuned\"\n",
    "# HF_TOKEN = \"\"\n",
    "\n",
    "def train(model, processor, train_dataloader, valid_dataloader, num_epochs, resume_training=False, patience=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    start_epoch = 0\n",
    "    min_eval_loss = float('inf')\n",
    "    early_stopping_hook = 0\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device=\"cuda\") if device.type == \"cuda\" else None\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if device.type == \"cuda\":\n",
    "                with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                    outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n",
    "                                    attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n",
    "                                attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        examples = []\n",
    "        preds_list = []\n",
    "        refs_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_dataloader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\") if device.type == \"cuda\" else torch.no_grad():\n",
    "                    outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n",
    "                                    attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    generated_ids = model.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=20\n",
    "                    )\n",
    "\n",
    "                preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                refs = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "                for p, r in zip(preds, refs):\n",
    "                    examples.append({'pred': p.strip(), 'ref': r.strip()})\n",
    "                    preds_list.append(p.strip())\n",
    "                    refs_list.append(r.strip())\n",
    "\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
    "\n",
    "        # Calculate BERTScore for entire validation set predictions vs references\n",
    "        P, R, F1 = bert_score(preds_list, refs_list, lang=\"en\", device=device)\n",
    "        avg_bertscore_f1 = F1.mean().item()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} Metrics:\")\n",
    "        print(f\"Train Loss: {avg_epoch_loss:.4f} | Eval Loss: {avg_eval_loss:.4f}\")\n",
    "        print(f\"BERTScore F1: {avg_bertscore_f1:.4f}\")\n",
    "\n",
    "        print(\"Sample predictions:\")\n",
    "        for i, ex in enumerate(examples[:5]):\n",
    "            print(f\"  {i+1}. Pred: '{ex['pred']}', Ref: '{ex['ref']}'\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save model checkpoint\n",
    "        subfolder = f\"epoch-{epoch+1}\"\n",
    "        os.makedirs(subfolder, exist_ok=True)\n",
    "        model.save_pretrained(subfolder)\n",
    "\n",
    "        api.upload_folder(\n",
    "            folder_path=subfolder,\n",
    "            path_in_repo=subfolder,\n",
    "            repo_id=REPO_ID,\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "\n",
    "        # Save metrics\n",
    "        metrics_data = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_epoch_loss,\n",
    "            \"eval_loss\": avg_eval_loss,\n",
    "            \"bertscore_f1\": avg_bertscore_f1,\n",
    "            \"examples\": examples[:5]\n",
    "        }\n",
    "\n",
    "        metrics_path = f\"{subfolder}/metrics.json\"\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump(metrics_data, f, indent=2)\n",
    "\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=metrics_path,\n",
    "            path_in_repo=f\"{subfolder}/metrics.json\",\n",
    "            repo_id=REPO_ID,\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "\n",
    "        print(f\"Pushed model and metrics for epoch {epoch+1}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_eval_loss < min_eval_loss:\n",
    "            model.push_to_hub(REPO_ID, commit_message=f\"Best model at epoch {epoch+1}\")\n",
    "            processor.push_to_hub(REPO_ID)\n",
    "            min_eval_loss = avg_eval_loss\n",
    "            early_stopping_hook = 0\n",
    "            print(\"New best model pushed.\")\n",
    "        else:\n",
    "            early_stopping_hook += 1\n",
    "            if early_stopping_hook > patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:47:29.511181Z",
     "iopub.status.busy": "2025-05-15T09:47:29.510588Z",
     "iopub.status.idle": "2025-05-15T09:52:06.478823Z",
     "shell.execute_reply": "2025-05-15T09:52:06.477704Z",
     "shell.execute_reply.started": "2025-05-15T09:47:29.511156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/30: 100%|██████████| 4/4 [00:30<00:00,  7.55s/it]\n",
      "Validation Epoch 1/30: 100%|██████████| 1/1 [00:14<00:00, 14.83s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf5a3b0754d4e47b749531692b189d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b6500c2ae94298bb81acdba9bc5b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0968176070614d92b59e15059c89af14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1975e4f36a64bbc8b34ef6a0943c711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ec2350bc984c93a6078b587dce62e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf49f8777984031b154165cb4a939e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Metrics:\n",
      "Train Loss: 9.4015 | Eval Loss: 9.3733\n",
      "BERTScore F1: 0.9811\n",
      "Sample predictions:\n",
      "  1. Pred: 'yes', Ref: 'hearts'\n",
      "  2. Pred: 'green', Ref: 'blue'\n",
      "  3. Pred: 'curved', Ref: 'rectangular'\n",
      "  4. Pred: 'yes', Ref: 'water'\n",
      "  5. Pred: 'no', Ref: 'no'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48181a881ccf4743826e1d5c482baa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed model and metrics for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model pushed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/30: 100%|██████████| 4/4 [00:29<00:00,  7.46s/it]\n",
      "Validation Epoch 2/30: 100%|██████████| 1/1 [00:14<00:00, 14.84s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Metrics:\n",
      "Train Loss: 9.3655 | Eval Loss: 9.3248\n",
      "BERTScore F1: 0.9811\n",
      "Sample predictions:\n",
      "  1. Pred: '6', Ref: '4'\n",
      "  2. Pred: 'yes', Ref: 'plastic'\n",
      "  3. Pred: 'white', Ref: 'white'\n",
      "  4. Pred: 'yes', Ref: 'blue'\n",
      "  5. Pred: 'yes', Ref: 'same'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722ba43cd10140c5875fefd32f8ff576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed model and metrics for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model pushed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/30: 100%|██████████| 4/4 [00:29<00:00,  7.41s/it]\n",
      "Validation Epoch 3/30: 100%|██████████| 1/1 [00:14<00:00, 14.66s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Metrics:\n",
      "Train Loss: 9.3203 | Eval Loss: 9.2749\n",
      "BERTScore F1: 0.9811\n",
      "Sample predictions:\n",
      "  1. Pred: 'metal', Ref: 'glass'\n",
      "  2. Pred: '2', Ref: 'one'\n",
      "  3. Pred: 'yes', Ref: 'white'\n",
      "  4. Pred: 'green', Ref: 'green'\n",
      "  5. Pred: 'curved', Ref: 'angled'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f63ef30e1f460db29f2d3733ba8ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed model and metrics for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model pushed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/30: 100%|██████████| 4/4 [00:29<00:00,  7.37s/it]\n",
      "Validation Epoch 4/30: 100%|██████████| 1/1 [00:14<00:00, 14.66s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Metrics:\n",
      "Train Loss: 9.2707 | Eval Loss: 9.2303\n",
      "BERTScore F1: 0.9811\n",
      "Sample predictions:\n",
      "  1. Pred: 'rectangular', Ref: 'rectangular'\n",
      "  2. Pred: 'no', Ref: 'sitting'\n",
      "  3. Pred: 'yes', Ref: 'plastic'\n",
      "  4. Pred: 'hand holding', Ref: 'front'\n",
      "  5. Pred: '4', Ref: 'six'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1c5e1bf2b248daa1a555d9e32776a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed model and metrics for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model pushed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/30: 100%|██████████| 4/4 [00:29<00:00,  7.44s/it]\n",
      "Validation Epoch 5/30: 100%|██████████| 1/1 [00:14<00:00, 14.65s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Metrics:\n",
      "Train Loss: 9.2270 | Eval Loss: 9.1961\n",
      "BERTScore F1: 0.9810\n",
      "Sample predictions:\n",
      "  1. Pred: 'blue', Ref: 'blue'\n",
      "  2. Pred: 'yes', Ref: 'sunset'\n",
      "  3. Pred: 'no', Ref: 'vertical'\n",
      "  4. Pred: 'black', Ref: 'black'\n",
      "  5. Pred: 'no', Ref: 'slip'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baca2e95daf5427f847890c792697ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed model and metrics for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model pushed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/30:  25%|██▌       | 1/4 [00:10<00:31, 10.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/27146492.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m train(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlora_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/2495581306.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, processor, train_dataloader, valid_dataloader, num_epochs, resume_training, patience)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n\u001b[0m\u001b[1;32m     42\u001b[0m                                     attention_mask=attention_mask, labels=labels)\n\u001b[1;32m     43\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, decoder_input_ids, decoder_attention_mask, attention_mask, output_attentions, output_hidden_states, labels, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0mimage_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         question_embeds = self.text_encoder(\n\u001b[0m\u001b[1;32m   1336\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder)\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 )\n\u001b[1;32m    435\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    437\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             cross_attention_outputs = self.crossattention(\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 275\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BlipTextModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "NUM_EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "\n",
    "# Start training\n",
    "train(\n",
    "    model=lora_model,\n",
    "    processor=processor,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    resume_training=True,\n",
    "    patience=PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:38:31.990881Z",
     "iopub.status.busy": "2025-05-15T09:38:31.990333Z",
     "iopub.status.idle": "2025-05-15T09:38:32.149679Z",
     "shell.execute_reply": "2025-05-15T09:38:32.149151Z",
     "shell.execute_reply.started": "2025-05-15T09:38:31.990854Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from huggingface_hub import HfApi, login\n",
    "import pickle\n",
    "\n",
    "\n",
    "login(token=\"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\")\n",
    "repo_id = \"adityaav80/blip-basevqa-finetuned\"\n",
    "api = HfApi()\n",
    "\n",
    "def train(model, processor, train_dataloader, valid_dataloader, num_epochs, resume_training=False, patience=3):\n",
    "\n",
    "    # Where to save the best models\n",
    "    best_model_repo = \"adityaav80/blip-basevqa-finetuned\"\n",
    "\n",
    "    # Initialising epoch\n",
    "    # Initialising the lowest validation loss\n",
    "    # Initialising how may times validation loss has not imporoved\n",
    "    \n",
    "    start_epoch = 0\n",
    "    min_eval_loss = float('inf')\n",
    "    early_stopping_hook = 0\n",
    "\n",
    "    # Load the latest checkpoint if resuming training\n",
    "    if resume_training:\n",
    "        \n",
    "        latest_checkpoint_path = get_latest_checkpoint_from_hf(\"adityaav80/blip-basevqa-finetuned\",\"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\")\n",
    "        \n",
    "        if latest_checkpoint_path:\n",
    "            \n",
    "            print(f\"Resuming from checkpoint at {latest_checkpoint_path}\")\n",
    "            base_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints\\\\')\n",
    "            processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints\\\\')\n",
    "            model = PeftModel.from_pretrained(base_model, f\"https://huggingface.co/adityaav80/blip-basevqa-finetuned/{latest_checkpoint_path}\")\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            model.to(device)\n",
    "\n",
    "            # Set all params to non-trainable first\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            ## Unset few parameters which have \"lora\"\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"lora\" in name:  # Customize based on LoRA layers naming convention\n",
    "                    param.requires_grad = True\n",
    "            model.print_trainable_parameters()\n",
    "\n",
    "            # get latest epoch from checkpint folder            \n",
    "            start_epoch = int(latest_checkpoint_path.split('checkpoint')[-1])\n",
    "\n",
    "    # GradScaler helps avoid numerical problems during backprop by scaling gradients.\n",
    "    # This creates a gradient scaler for mixed precision training on CUDA.\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")\n",
    "    # Just an empty list to keep track of losses and metrics during training.\n",
    "    tracking_information = []\n",
    "\n",
    "    # set optimizer as adam with weight decay and trainable parameters\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    # this enables weight decay\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
    "\n",
    "    ########################## backward propagation #################################3\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "        ## track epoch_loss\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n",
    "            \n",
    "            ## get the output from __get__item from VQADataset class\n",
    "            input_ids = batch.pop('input_ids').to(model.device)\n",
    "            pixel_values = batch.pop('pixel_values').to(model.device)\n",
    "            attention_mask = batch.pop('attention_mask').to(model.device)\n",
    "            labels = batch.pop('labels').to(model.device)\n",
    "\n",
    "            \"\"\"\n",
    "            Usually we declare loss like this in the main function\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            But HuggingFace comes with default built in loss function \n",
    "            you use .loss to access it\n",
    "            \"\"\"\n",
    "\n",
    "            # clear previous iterations results before backward prop\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            # for current epoch\n",
    "            epoch_loss += loss.item()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        ## total loss per batch / batch size \n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        ## track information\n",
    "        tracking_information.append({'epoch': epoch + 1, 'train_loss': avg_epoch_loss})\n",
    "\n",
    "        ##########################################################################\n",
    "\n",
    "        ############################# evaluation of model ########################\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_dataloader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n",
    "                input_ids = batch.pop('input_ids').to(model.device)\n",
    "                pixel_values = batch.pop('pixel_values').to(model.device)\n",
    "                attention_mask = batch.pop('attention_mask').to(model.device)\n",
    "                labels = batch.pop('labels').to(model.device)\n",
    "\n",
    "                with torch.amp.autocast(\"cuda\"):\n",
    "                    outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    eval_loss += loss.item()\n",
    "\n",
    "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
    "        tracking_information[-1]['eval_loss'] = avg_eval_loss\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_epoch_loss}, Validation Loss = {avg_eval_loss}\")\n",
    "        scheduler.step()\n",
    "\n",
    "        ######################################################################\n",
    "\n",
    "        checkpoint_repo_id = f\"adityaav80/blip-basevqa-finetuned-checkpoint-epoch{epoch+1}\"\n",
    "        model.push_to_hub(checkpoint_repo_id, commit_message=f\"Checkpoint at epoch {epoch+1}\")\n",
    "        print(f\"Checkpoint pushed to Hugging Face Hub: {checkpoint_repo_id}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_eval_loss < min_eval_loss:\n",
    "            \n",
    "            model.push_to_hub(best_model_repo, commit_message=f\"Epoch{epoch}\")\n",
    "            processor.push_to_hub(best_model_repo)\n",
    "            min_eval_loss = avg_eval_loss\n",
    "            early_stopping_hook = 0\n",
    "            print(f\"New best model pushed to Hugging Face Hub: {best_model_repo}\")\n",
    "                        \n",
    "        else:\n",
    "            \n",
    "            early_stopping_hook += 1\n",
    "            if early_stopping_hook > patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    push_tracking_info_to_hub(tracking_information, best_model_repo, commit_message=f\"Tracking info epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU + GPU Variant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:38:56.705334Z",
     "iopub.status.busy": "2025-05-15T09:38:56.705038Z",
     "iopub.status.idle": "2025-05-15T09:38:56.822143Z",
     "shell.execute_reply": "2025-05-15T09:38:56.821478Z",
     "shell.execute_reply.started": "2025-05-15T09:38:56.705309Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from huggingface_hub import HfApi, login\n",
    "from peft import PeftModel\n",
    "from transformers import BlipForQuestionAnswering, BlipProcessor\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "login(token=\"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\")\n",
    "api = HfApi()\n",
    "\n",
    "def train(model, processor, train_dataloader, valid_dataloader, num_epochs, resume_training=False, patience=3):\n",
    "    best_model_repo = \"adityaav80/blip-basevqa-finetuned\"\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    start_epoch = 0\n",
    "    min_eval_loss = float('inf')\n",
    "    early_stopping_hook = 0\n",
    "\n",
    "    if resume_training:\n",
    "        latest_checkpoint_path = get_latest_checkpoint_from_hf(best_model_repo, \"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\")\n",
    "        if latest_checkpoint_path:\n",
    "            print(f\"Resuming from checkpoint at {latest_checkpoint_path}\")\n",
    "            base_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints/')\n",
    "            processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints/')\n",
    "            model = PeftModel.from_pretrained(base_model, f\"https://huggingface.co/{best_model_repo}/{latest_checkpoint_path}\")\n",
    "            model.to(device)\n",
    "\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"lora\" in name:\n",
    "                    param.requires_grad = True\n",
    "            model.print_trainable_parameters()\n",
    "\n",
    "            start_epoch = int(latest_checkpoint_path.split('checkpoint')[-1])\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if device.type == \"cuda\" else None\n",
    "    tracking_information = []\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n",
    "            input_ids = batch.pop('input_ids').to(device)\n",
    "            pixel_values = batch.pop('pixel_values').to(device)\n",
    "            attention_mask = batch.pop('attention_mask').to(device)\n",
    "            labels = batch.pop('labels').to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if device.type == \"cuda\":\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n",
    "                                    attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n",
    "                                attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        tracking_information.append({'epoch': epoch + 1, 'train_loss': avg_epoch_loss})\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_dataloader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n",
    "                input_ids = batch.pop('input_ids').to(device)\n",
    "                pixel_values = batch.pop('pixel_values').to(device)\n",
    "                attention_mask = batch.pop('attention_mask').to(device)\n",
    "                labels = batch.pop('labels').to(device)\n",
    "\n",
    "                if device.type == \"cuda\":\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n",
    "                                        attention_mask=attention_mask, labels=labels)\n",
    "                        loss = outputs.loss\n",
    "                else:\n",
    "                    outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n",
    "                                    attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
    "        tracking_information[-1]['eval_loss'] = avg_eval_loss\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_epoch_loss}, Validation Loss = {avg_eval_loss}\")\n",
    "        scheduler.step()\n",
    "\n",
    "        checkpoint_repo_id = f\"adityaav80/blip-basevqa-finetuned-checkpoint-epoch{epoch+1}\"\n",
    "        model.push_to_hub(checkpoint_repo_id, commit_message=f\"Checkpoint at epoch {epoch+1}\")\n",
    "        print(f\"Checkpoint pushed to Hugging Face Hub: {checkpoint_repo_id}\")\n",
    "\n",
    "        if avg_eval_loss < min_eval_loss:\n",
    "            model.push_to_hub(best_model_repo, commit_message=f\"Epoch{epoch}\")\n",
    "            processor.push_to_hub(best_model_repo)\n",
    "            min_eval_loss = avg_eval_loss\n",
    "            early_stopping_hook = 0\n",
    "            print(f\"New best model pushed to Hugging Face Hub: {best_model_repo}\")\n",
    "        else:\n",
    "            early_stopping_hook += 1\n",
    "            if early_stopping_hook > patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    push_tracking_info_to_hub(tracking_information, best_model_repo, commit_message=f\"Tracking info epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:45:29.673867Z",
     "iopub.status.busy": "2025-05-15T09:45:29.673173Z",
     "iopub.status.idle": "2025-05-15T09:45:29.677650Z",
     "shell.execute_reply": "2025-05-15T09:45:29.676752Z",
     "shell.execute_reply.started": "2025-05-15T09:45:29.673832Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from peft import PeftModel\n",
    "# NUM_EPOCHS = 20\n",
    "# PATIENCE = 3\n",
    "\n",
    "# # Start training\n",
    "# train(\n",
    "#     model=lora_model,\n",
    "#     processor=processor,\n",
    "#     train_dataloader=train_dataloader,\n",
    "#     valid_dataloader=valid_dataloader,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     resume_training=True,\n",
    "#     patience=PATIENCE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12309846,
     "datasetId": 7423275,
     "sourceId": 11818310,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 12260662,
     "datasetId": 7392738,
     "sourceId": 11775052,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
