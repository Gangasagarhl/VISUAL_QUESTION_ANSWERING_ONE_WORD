{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":11818310,"datasetId":7423275,"databundleVersionId":12309846},{"sourceType":"datasetVersion","sourceId":11775052,"datasetId":7392738,"databundleVersionId":12260662}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:53:18.284747Z","iopub.execute_input":"2025-05-15T08:53:18.285247Z","iopub.status.idle":"2025-05-15T08:53:22.471226Z","shell.execute_reply.started":"2025-05-15T08:53:18.285218Z","shell.execute_reply":"2025-05-15T08:53:22.470418Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom PIL import Image\n\nclass VQADataset(torch.utils.data.Dataset):\n\n    def __init__(self, dataset_load, processor_load):\n        self.dataset = dataset_load\n        self.processor = processor_load\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        question = self.dataset[idx]['question']\n        answer = self.dataset[idx]['answer']\n        image_id = self.dataset[idx]['image_id']\n        image_path = self.dataset[idx]['image_path']\n        image = Image.open(image_path).convert(\"RGB\")\n        text = question\n\n        \"\"\"Model Prediction\"\"\"\n\n        \"\"\"\n        Image - Image loaded from csv file\n        Text - the question loaded from csv file\n        Padding - set to max length of the model - BlipProcessor set's this limit\n        Truncation - If the question is too long, it cuts off extra tokens to fit max length.\n        Return Tensors - Return the output as PyTorch tensors (default is list of ints).\n        Return Attention Mask - Tells the model which tokens are actual input and which are padding\n\n        Encoding returns\n        {pixel_values:, input_ids:, attention_mask}\n        pixel_values - raw pixel values of imafe\n        input_ids - input model\n        attention_mask - Returns a tensor like [1, 1, 1, 0, 0]\n        \"\"\"\n        \n        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\", return_attention_mask=True)\n        \n        \"\"\"Encoding our Answers\"\"\"\n        labels = self.processor.tokenizer.encode(\n            str(answer),\n            max_length= 16,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors='pt',\n            return_attention_mask=True\n        )\n\n        # Add labels also to the dictionary\n        encoding[\"labels\"] = labels\n\n        # Remove extra dimension from the attention mask\n        encoding[\"attention_mask\"] = encoding[\"attention_mask\"].squeeze()\n        \n        # Remove batch dimension\n        for dict_key,dict_value in encoding.items():  \n            encoding[dict_key] = dict_value.squeeze()\n            \n        return encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:53:22.472564Z","iopub.execute_input":"2025-05-15T08:53:22.472870Z","iopub.status.idle":"2025-05-15T08:53:22.480842Z","shell.execute_reply.started":"2025-05-15T08:53:22.472850Z","shell.execute_reply":"2025-05-15T08:53:22.479940Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering\n\n# cached_dir = os.path.join('BLIP_checkpoints')\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")#,cache_dir=cached_dir)\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\",use_fast = True)#,cache_dir=cached_dir)\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:53:22.481466Z","iopub.execute_input":"2025-05-15T08:53:22.481660Z","iopub.status.idle":"2025-05-15T08:53:53.838813Z","shell.execute_reply.started":"2025-05-15T08:53:22.481622Z","shell.execute_reply":"2025-05-15T08:53:53.838164Z"}},"outputs":[{"name":"stderr","text":"2025-05-15 08:53:32.141374: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747299212.330882      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747299212.388615      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df43473a582a4e049eb5a0bc8b6894d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28509cead2054246bc2547903098ba04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10e428b8e764a9b8409fb9035472759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6c2a7a5529246f8bed2e5146670223b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83499927b28641508d26780ce33ba86d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510a169cda404b369fde824e174bd93a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e5a9db69b349d0837edc22c56a7a91"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x79392fb10610>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import os\nimport kagglehub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset, Dataset\n\nkagglehub.dataset_download(\"hlgsagar1234567/vr-go\")\nkagglehub.dataset_download(\"rajan56/datacuration\")\n\ntrain_df = pd.read_csv(\"/kaggle/input/datacuration/data_curation_train.csv\")  \nval_df = pd.read_csv(\"/kaggle/input/datacuration/data_curation_val.csv\")\n\ntrain_df = train_df.sample(n=512, random_state=42).reset_index(drop=True)\nval_df = val_df.sample(n=128, random_state=42).reset_index(drop=True)\n\nprint(len(train_df),len(val_df))\n\n# Convert back to Hugging Face Datasets\ntraining_data = Dataset.from_pandas(train_df)\nvalid_data = Dataset.from_pandas(val_df)\n\ntraining_set = VQADataset(dataset_load=training_data, processor_load=processor)\nvalid_set = VQADataset(dataset_load=valid_data, processor_load=processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:38:04.814655Z","iopub.execute_input":"2025-05-15T09:38:04.814932Z","iopub.status.idle":"2025-05-15T09:38:06.196992Z","shell.execute_reply.started":"2025-05-15T09:38:04.814908Z","shell.execute_reply":"2025-05-15T09:38:06.196284Z"}},"outputs":[{"name":"stdout","text":"512 128\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"\"\"\"Visualising data returned from Dataset, VQA Dataset \"\"\"\n# print(training_data.column_names)\n# for idx in range(1):\n#     encoding = training_set[idx]\n#     print(\"Encoding\",encoding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:18:15.623427Z","iopub.execute_input":"2025-05-15T09:18:15.623878Z","iopub.status.idle":"2025-05-15T09:18:15.628243Z","shell.execute_reply.started":"2025-05-15T09:18:15.623854Z","shell.execute_reply":"2025-05-15T09:18:15.627532Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'Visualising data returned from Dataset, VQA Dataset '"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nBATCH_SIZE = 128\n\n\"\"\"\nThe DataLoader + collate_fn work on your processed dataset items (training_set) \n\ntraining_set[idx] calls your __getitem__ method, which:\nLoads the image,\nProcesses it with the processor to create tensors (input_ids, pixel_values, attention_mask),\nEncodes labels,\nReturns a dictionary of tensors\n\"\"\"\n\ndef custom_collate(batch):\n    \n    # Separate each element in the batch\n    input_ids = [item[\"input_ids\"] for item in batch]\n    pixel_values = [item[\"pixel_values\"] for item in batch]\n    attention_mask = [item[\"attention_mask\"] for item in batch]\n    labels = [item[\"labels\"] for item in batch]\n\n    \"\"\"input_ids\"\"\"\n    # Pad sequences to the maximum length in the batch\n    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)  # Adjust padding_value as needed\n\n    \"\"\"labels\"\"\"\n    # The special value -100 is the default ignore_index in PyTorch’s CrossEntropyLoss.\n    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n\n    \"\"\"pixel_values\"\"\"\n    \"\"\"attention_mask\"\"\"\n    # stacks all items -> [3, H, W] => [batch_size, 3, H, W]\n    pixel_values_stacked = torch.stack(pixel_values)  \n    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n\n    # Return the batch as a dictionary\n    return {\n        \"input_ids\": input_ids_padded,\n        \"pixel_values\": pixel_values_stacked,\n        \"attention_mask\": attention_mask_padded,\n        \"labels\": labels_padded,\n    }\n\n# Use this collate function in your DataLoader\ntrain_dataloader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\nvalid_dataloader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:38:07.893814Z","iopub.execute_input":"2025-05-15T09:38:07.894449Z","iopub.status.idle":"2025-05-15T09:38:07.900800Z","shell.execute_reply.started":"2025-05-15T09:38:07.894419Z","shell.execute_reply":"2025-05-15T09:38:07.900017Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# for row in train_dataloader:\n#     for k,v in row.items():\n#         print(k)\n#     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:18:22.887363Z","iopub.execute_input":"2025-05-15T09:18:22.887885Z","iopub.status.idle":"2025-05-15T09:18:22.890901Z","shell.execute_reply.started":"2025-05-15T09:18:22.887860Z","shell.execute_reply":"2025-05-15T09:18:22.890239Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# for name, module in model.named_modules():\n#     print(name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:53:58.382682Z","iopub.execute_input":"2025-05-15T08:53:58.382887Z","iopub.status.idle":"2025-05-15T08:53:58.386354Z","shell.execute_reply.started":"2025-05-15T08:53:58.382869Z","shell.execute_reply":"2025-05-15T08:53:58.385658Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"text_encoder_target_modules = [\n    f\"text_decoder.bert.encoder.layer.{i}.crossattention.self.query\" for i in range(1, 12)\n] + [\n    f\"text_decoder.bert.encoder.layer.{i}.crossattention.self.value\" for i in range(1, 12)\n]\n\nprint(text_encoder_target_modules)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:38:10.941735Z","iopub.execute_input":"2025-05-15T09:38:10.942462Z","iopub.status.idle":"2025-05-15T09:38:10.946952Z","shell.execute_reply.started":"2025-05-15T09:38:10.942433Z","shell.execute_reply":"2025-05-15T09:38:10.946335Z"}},"outputs":[{"name":"stdout","text":"['text_decoder.bert.encoder.layer.1.crossattention.self.query', 'text_decoder.bert.encoder.layer.2.crossattention.self.query', 'text_decoder.bert.encoder.layer.3.crossattention.self.query', 'text_decoder.bert.encoder.layer.4.crossattention.self.query', 'text_decoder.bert.encoder.layer.5.crossattention.self.query', 'text_decoder.bert.encoder.layer.6.crossattention.self.query', 'text_decoder.bert.encoder.layer.7.crossattention.self.query', 'text_decoder.bert.encoder.layer.8.crossattention.self.query', 'text_decoder.bert.encoder.layer.9.crossattention.self.query', 'text_decoder.bert.encoder.layer.10.crossattention.self.query', 'text_decoder.bert.encoder.layer.11.crossattention.self.query', 'text_decoder.bert.encoder.layer.1.crossattention.self.value', 'text_decoder.bert.encoder.layer.2.crossattention.self.value', 'text_decoder.bert.encoder.layer.3.crossattention.self.value', 'text_decoder.bert.encoder.layer.4.crossattention.self.value', 'text_decoder.bert.encoder.layer.5.crossattention.self.value', 'text_decoder.bert.encoder.layer.6.crossattention.self.value', 'text_decoder.bert.encoder.layer.7.crossattention.self.value', 'text_decoder.bert.encoder.layer.8.crossattention.self.value', 'text_decoder.bert.encoder.layer.9.crossattention.self.value', 'text_decoder.bert.encoder.layer.10.crossattention.self.value', 'text_decoder.bert.encoder.layer.11.crossattention.self.value']\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=8, # rank of the low-rank matrices that LoRA uses to modify certain parts of the model\n    lora_alpha=32, # scale LoRA weights, effectively controlling how strongly the LoRA layers impact the model’s predictions\n    lora_dropout=0.05, #\n    bias=\"none\", # whether or not to add biases to the LoRA-modified layers\n    target_modules=text_encoder_target_modules      # specifies the exact layers in the model where LoRA should be applied, typically in attention mechanisms\n                                        # [\"q_proj\", \"k_proj\"] are standard as they affect the query and key projections in attention layers\n)\n\nlora_model = get_peft_model(model, config)\nlora_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:38:11.820249Z","iopub.execute_input":"2025-05-15T09:38:11.820827Z","iopub.status.idle":"2025-05-15T09:38:11.866710Z","shell.execute_reply.started":"2025-05-15T09:38:11.820800Z","shell.execute_reply":"2025-05-15T09:38:11.866165Z"}},"outputs":[{"name":"stdout","text":"trainable params: 270,336 || all params: 384,942,908 || trainable%: 0.0702\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## Sagar's Variant","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi\ndef get_latest_checkpoint_from_hf(repo_id, token=None):\n    \"\"\"\n    Fetch the latest checkpoint folder name or path from a Hugging Face repo.\n\n    repo_id: str, e.g. \"username/modelname\"\n    token: Optional str, your HF token if private repo\n\n    Returns:\n    latest_checkpoint: str or None\n    \"\"\"\n\n    login(token=token)\n    api = HfApi()\n    # List all files in the repo root (or a specific folder if you organize checkpoints)\n    files = api.list_repo_files(repo_id=repo_id, token=token)\n\n    # Filter checkpoint folders/files\n    checkpoints = [f for f in files if f.startswith('checkpoint')]\n\n    if not checkpoints:\n        return None\n\n    # Assuming checkpoints have incremental naming like checkpoint1, checkpoint2...\n    # Sort them based on numeric suffix\n    checkpoints.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n\n    return checkpoints[-1] \n\ndef push_tracking_info_to_hub(tracking_info, repo_id, commit_message=\"Update tracking info\"):\n    with io.BytesIO() as f:\n        pickle.dump(tracking_info, f)\n        f.seek(0)\n        api.upload_file(\n            path_or_fileobj=f,\n            path_in_repo=\"tracking_info.pkl\",\n            repo_id=repo_id,\n            repo_type=\"model\",\n            commit_message=commit_message,\n            token=\"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\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:38:13.988838Z","iopub.execute_input":"2025-05-15T09:38:13.989392Z","iopub.status.idle":"2025-05-15T09:38:13.995099Z","shell.execute_reply.started":"2025-05-15T09:38:13.989367Z","shell.execute_reply":"2025-05-15T09:38:13.994366Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"!pip install bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:40:27.320930Z","iopub.execute_input":"2025-05-15T09:40:27.321479Z","iopub.status.idle":"2025-05-15T09:41:40.334087Z","shell.execute_reply.started":"2025-05-15T09:40:27.321452Z","shell.execute_reply":"2025-05-15T09:41:40.333341Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.31.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.4.26)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert_score) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"from tqdm import tqdm\nfrom huggingface_hub import HfApi, login\nfrom peft import PeftModel\nfrom transformers import BlipForQuestionAnswering, BlipProcessor\nimport torch\nimport os\nimport json\nfrom bert_score import score as bert_score\n\nlogin(token=\"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\")\napi = HfApi()\nREPO_ID = \"adityaav80/blip-basevqa-finetuned\"\nHF_TOKEN = \"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\"\n\ndef train(model, processor, train_dataloader, valid_dataloader, num_epochs, resume_training=False, patience=3):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    start_epoch = 0\n    min_eval_loss = float('inf')\n    early_stopping_hook = 0\n\n    scaler = torch.amp.GradScaler(device=\"cuda\") if device.type == \"cuda\" else None\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        epoch_loss = 0\n\n        for batch in tqdm(train_dataloader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n            input_ids = batch['input_ids'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            optimizer.zero_grad()\n\n            if device.type == \"cuda\":\n                with torch.amp.autocast(device_type=\"cuda\"):\n                    outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n                                    attention_mask=attention_mask, labels=labels)\n                    loss = outputs.loss\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n                                attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                loss.backward()\n                optimizer.step()\n\n            epoch_loss += loss.item()\n\n        avg_epoch_loss = epoch_loss / len(train_dataloader)\n\n        # Evaluation\n        model.eval()\n        eval_loss = 0\n        examples = []\n        preds_list = []\n        refs_list = []\n\n        with torch.no_grad():\n            for batch in tqdm(valid_dataloader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n                input_ids = batch['input_ids'].to(device)\n                pixel_values = batch['pixel_values'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n\n                with torch.amp.autocast(device_type=\"cuda\") if device.type == \"cuda\" else torch.no_grad():\n                    outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n                                    attention_mask=attention_mask, labels=labels)\n                    loss = outputs.loss\n\n                    generated_ids = model.generate(\n                        input_ids=input_ids,\n                        pixel_values=pixel_values,\n                        attention_mask=attention_mask,\n                        max_length=20\n                    )\n\n                preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n                refs = processor.batch_decode(labels, skip_special_tokens=True)\n\n                for p, r in zip(preds, refs):\n                    examples.append({'pred': p.strip(), 'ref': r.strip()})\n                    preds_list.append(p.strip())\n                    refs_list.append(r.strip())\n\n                eval_loss += loss.item()\n\n        avg_eval_loss = eval_loss / len(valid_dataloader)\n\n        # Calculate BERTScore for entire validation set predictions vs references\n        P, R, F1 = bert_score(preds_list, refs_list, lang=\"en\", device=device)\n        avg_bertscore_f1 = F1.mean().item()\n\n        print(f\"\\nEpoch {epoch+1} Metrics:\")\n        print(f\"Train Loss: {avg_epoch_loss:.4f} | Eval Loss: {avg_eval_loss:.4f}\")\n        print(f\"BERTScore F1: {avg_bertscore_f1:.4f}\")\n\n        print(\"Sample predictions:\")\n        for i, ex in enumerate(examples[:5]):\n            print(f\"  {i+1}. Pred: '{ex['pred']}', Ref: '{ex['ref']}'\")\n\n        scheduler.step()\n\n        # Save model checkpoint\n        subfolder = f\"epoch-{epoch+1}\"\n        os.makedirs(subfolder, exist_ok=True)\n        model.save_pretrained(subfolder)\n\n        api.upload_folder(\n            folder_path=subfolder,\n            path_in_repo=subfolder,\n            repo_id=REPO_ID,\n            token=HF_TOKEN\n        )\n\n        # Save metrics\n        metrics_data = {\n            \"epoch\": epoch + 1,\n            \"train_loss\": avg_epoch_loss,\n            \"eval_loss\": avg_eval_loss,\n            \"bertscore_f1\": avg_bertscore_f1,\n            \"examples\": examples[:5]\n        }\n\n        metrics_path = f\"{subfolder}/metrics.json\"\n        with open(metrics_path, \"w\") as f:\n            json.dump(metrics_data, f, indent=2)\n\n        api.upload_file(\n            path_or_fileobj=metrics_path,\n            path_in_repo=f\"{subfolder}/metrics.json\",\n            repo_id=REPO_ID,\n            token=HF_TOKEN\n        )\n\n        print(f\"Pushed model and metrics for epoch {epoch+1}\")\n\n        # Save best model\n        if avg_eval_loss < min_eval_loss:\n            model.push_to_hub(REPO_ID, commit_message=f\"Best model at epoch {epoch+1}\")\n            processor.push_to_hub(REPO_ID)\n            min_eval_loss = avg_eval_loss\n            early_stopping_hook = 0\n            print(\"New best model pushed.\")\n        else:\n            early_stopping_hook += 1\n            if early_stopping_hook > patience:\n                print(\"Early stopping triggered.\")\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:47:26.767033Z","iopub.execute_input":"2025-05-15T09:47:26.767356Z","iopub.status.idle":"2025-05-15T09:47:26.887504Z","shell.execute_reply.started":"2025-05-15T09:47:26.767323Z","shell.execute_reply":"2025-05-15T09:47:26.886920Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from peft import PeftModel\nNUM_EPOCHS = 30\nPATIENCE = 5\n\n# Start training\ntrain(\n    model=lora_model,\n    processor=processor,\n    train_dataloader=train_dataloader,\n    valid_dataloader=valid_dataloader,\n    num_epochs=NUM_EPOCHS,\n    resume_training=True,\n    patience=PATIENCE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:47:29.510588Z","iopub.execute_input":"2025-05-15T09:47:29.511181Z","iopub.status.idle":"2025-05-15T09:52:06.478823Z","shell.execute_reply.started":"2025-05-15T09:47:29.511156Z","shell.execute_reply":"2025-05-15T09:52:06.477704Z"}},"outputs":[{"name":"stderr","text":"Training Epoch 1/30: 100%|██████████| 4/4 [00:30<00:00,  7.55s/it]\nValidation Epoch 1/30: 100%|██████████| 1/1 [00:14<00:00, 14.83s/it]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbf5a3b0754d4e47b749531692b189d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b6500c2ae94298bb81acdba9bc5b8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0968176070614d92b59e15059c89af14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1975e4f36a64bbc8b34ef6a0943c711"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41ec2350bc984c93a6078b587dce62e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf49f8777984031b154165cb4a939e4"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Metrics:\nTrain Loss: 9.4015 | Eval Loss: 9.3733\nBERTScore F1: 0.9811\nSample predictions:\n  1. Pred: 'yes', Ref: 'hearts'\n  2. Pred: 'green', Ref: 'blue'\n  3. Pred: 'curved', Ref: 'rectangular'\n  4. Pred: 'yes', Ref: 'water'\n  5. Pred: 'no', Ref: 'no'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48181a881ccf4743826e1d5c482baa56"}},"metadata":{}},{"name":"stdout","text":"Pushed model and metrics for epoch 1\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"New best model pushed.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2/30: 100%|██████████| 4/4 [00:29<00:00,  7.46s/it]\nValidation Epoch 2/30: 100%|██████████| 1/1 [00:14<00:00, 14.84s/it]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 Metrics:\nTrain Loss: 9.3655 | Eval Loss: 9.3248\nBERTScore F1: 0.9811\nSample predictions:\n  1. Pred: '6', Ref: '4'\n  2. Pred: 'yes', Ref: 'plastic'\n  3. Pred: 'white', Ref: 'white'\n  4. Pred: 'yes', Ref: 'blue'\n  5. Pred: 'yes', Ref: 'same'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"722ba43cd10140c5875fefd32f8ff576"}},"metadata":{}},{"name":"stdout","text":"Pushed model and metrics for epoch 2\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"New best model pushed.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3/30: 100%|██████████| 4/4 [00:29<00:00,  7.41s/it]\nValidation Epoch 3/30: 100%|██████████| 1/1 [00:14<00:00, 14.66s/it]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 Metrics:\nTrain Loss: 9.3203 | Eval Loss: 9.2749\nBERTScore F1: 0.9811\nSample predictions:\n  1. Pred: 'metal', Ref: 'glass'\n  2. Pred: '2', Ref: 'one'\n  3. Pred: 'yes', Ref: 'white'\n  4. Pred: 'green', Ref: 'green'\n  5. Pred: 'curved', Ref: 'angled'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f63ef30e1f460db29f2d3733ba8ea6"}},"metadata":{}},{"name":"stdout","text":"Pushed model and metrics for epoch 3\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"New best model pushed.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4/30: 100%|██████████| 4/4 [00:29<00:00,  7.37s/it]\nValidation Epoch 4/30: 100%|██████████| 1/1 [00:14<00:00, 14.66s/it]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 Metrics:\nTrain Loss: 9.2707 | Eval Loss: 9.2303\nBERTScore F1: 0.9811\nSample predictions:\n  1. Pred: 'rectangular', Ref: 'rectangular'\n  2. Pred: 'no', Ref: 'sitting'\n  3. Pred: 'yes', Ref: 'plastic'\n  4. Pred: 'hand holding', Ref: 'front'\n  5. Pred: '4', Ref: 'six'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c1c5e1bf2b248daa1a555d9e32776a5"}},"metadata":{}},{"name":"stdout","text":"Pushed model and metrics for epoch 4\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"New best model pushed.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5/30: 100%|██████████| 4/4 [00:29<00:00,  7.44s/it]\nValidation Epoch 5/30: 100%|██████████| 1/1 [00:14<00:00, 14.65s/it]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 Metrics:\nTrain Loss: 9.2270 | Eval Loss: 9.1961\nBERTScore F1: 0.9810\nSample predictions:\n  1. Pred: 'blue', Ref: 'blue'\n  2. Pred: 'yes', Ref: 'sunset'\n  3. Pred: 'no', Ref: 'vertical'\n  4. Pred: 'black', Ref: 'black'\n  5. Pred: 'no', Ref: 'slip'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baca2e95daf5427f847890c792697ba7"}},"metadata":{}},{"name":"stdout","text":"Pushed model and metrics for epoch 5\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"New best model pushed.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6/30:  25%|██▌       | 1/4 [00:10<00:31, 10.57s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/27146492.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m train(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlora_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2495581306.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, processor, train_dataloader, valid_dataloader, num_epochs, resume_training, patience)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n\u001b[0m\u001b[1;32m     42\u001b[0m                                     attention_mask=attention_mask, labels=labels)\n\u001b[1;32m     43\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, decoder_input_ids, decoder_attention_mask, attention_mask, output_attentions, output_hidden_states, labels, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0mimage_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         question_embeds = self.text_encoder(\n\u001b[0m\u001b[1;32m   1336\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder)\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 )\n\u001b[1;32m    435\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    437\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             cross_attention_outputs = self.crossattention(\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 275\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BlipTextModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":41},{"cell_type":"markdown","source":"# CPU Variant","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom huggingface_hub import HfApi, login\nimport pickle\n\n\nlogin(token=\"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\")\nrepo_id = \"adityaav80/blip-basevqa-finetuned\"\napi = HfApi()\n\ndef train(model, processor, train_dataloader, valid_dataloader, num_epochs, resume_training=False, patience=3):\n\n    # Where to save the best models\n    best_model_repo = \"adityaav80/blip-basevqa-finetuned\"\n\n    # Initialising epoch\n    # Initialising the lowest validation loss\n    # Initialising how may times validation loss has not imporoved\n    \n    start_epoch = 0\n    min_eval_loss = float('inf')\n    early_stopping_hook = 0\n\n    # Load the latest checkpoint if resuming training\n    if resume_training:\n        \n        latest_checkpoint_path = get_latest_checkpoint_from_hf(\"adityaav80/blip-basevqa-finetuned\",\"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\")\n        \n        if latest_checkpoint_path:\n            \n            print(f\"Resuming from checkpoint at {latest_checkpoint_path}\")\n            base_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints\\\\')\n            processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints\\\\')\n            model = PeftModel.from_pretrained(base_model, f\"https://huggingface.co/adityaav80/blip-basevqa-finetuned/{latest_checkpoint_path}\")\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            \n            model.to(device)\n\n            # Set all params to non-trainable first\n            for param in model.parameters():\n                param.requires_grad = False\n\n            ## Unset few parameters which have \"lora\"\n            for name, param in model.named_parameters():\n                if \"lora\" in name:  # Customize based on LoRA layers naming convention\n                    param.requires_grad = True\n            model.print_trainable_parameters()\n\n            # get latest epoch from checkpint folder            \n            start_epoch = int(latest_checkpoint_path.split('checkpoint')[-1])\n\n    # GradScaler helps avoid numerical problems during backprop by scaling gradients.\n    # This creates a gradient scaler for mixed precision training on CUDA.\n    scaler = torch.amp.GradScaler(\"cuda\")\n    # Just an empty list to keep track of losses and metrics during training.\n    tracking_information = []\n\n    # set optimizer as adam with weight decay and trainable parameters\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n    # this enables weight decay\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n\n    ########################## backward propagation #################################3\n    \n    for epoch in range(start_epoch, num_epochs):\n\n        ## track epoch_loss\n        epoch_loss = 0\n        model.train()\n\n\n        for batch in tqdm(train_dataloader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n            \n            ## get the output from __get__item from VQADataset class\n            input_ids = batch.pop('input_ids').to(model.device)\n            pixel_values = batch.pop('pixel_values').to(model.device)\n            attention_mask = batch.pop('attention_mask').to(model.device)\n            labels = batch.pop('labels').to(model.device)\n\n            \"\"\"\n            Usually we declare loss like this in the main function\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(outputs, labels)\n\n            But HuggingFace comes with default built in loss function \n            you use .loss to access it\n            \"\"\"\n\n            # clear previous iterations results before backward prop\n            optimizer.zero_grad()\n\n            with torch.amp.autocast(\"cuda\"):\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n\n            # for current epoch\n            epoch_loss += loss.item()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n        ## total loss per batch / batch size \n        avg_epoch_loss = epoch_loss / len(train_dataloader)\n        ## track information\n        tracking_information.append({'epoch': epoch + 1, 'train_loss': avg_epoch_loss})\n\n        ##########################################################################\n\n        ############################# evaluation of model ########################\n\n        # Validation step\n        model.eval()\n        eval_loss = 0\n       \n        with torch.no_grad():\n            for batch in tqdm(valid_dataloader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n                input_ids = batch.pop('input_ids').to(model.device)\n                pixel_values = batch.pop('pixel_values').to(model.device)\n                attention_mask = batch.pop('attention_mask').to(model.device)\n                labels = batch.pop('labels').to(model.device)\n\n                with torch.amp.autocast(\"cuda\"):\n                    outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=labels)\n                    loss = outputs.loss\n                    eval_loss += loss.item()\n\n        avg_eval_loss = eval_loss / len(valid_dataloader)\n        tracking_information[-1]['eval_loss'] = avg_eval_loss\n        print(f\"Epoch {epoch+1}: Training Loss = {avg_epoch_loss}, Validation Loss = {avg_eval_loss}\")\n        scheduler.step()\n\n        ######################################################################\n\n        checkpoint_repo_id = f\"adityaav80/blip-basevqa-finetuned-checkpoint-epoch{epoch+1}\"\n        model.push_to_hub(checkpoint_repo_id, commit_message=f\"Checkpoint at epoch {epoch+1}\")\n        print(f\"Checkpoint pushed to Hugging Face Hub: {checkpoint_repo_id}\")\n\n        # Save best model\n        if avg_eval_loss < min_eval_loss:\n            \n            model.push_to_hub(best_model_repo, commit_message=f\"Epoch{epoch}\")\n            processor.push_to_hub(best_model_repo)\n            min_eval_loss = avg_eval_loss\n            early_stopping_hook = 0\n            print(f\"New best model pushed to Hugging Face Hub: {best_model_repo}\")\n                        \n        else:\n            \n            early_stopping_hook += 1\n            if early_stopping_hook > patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    push_tracking_info_to_hub(tracking_information, best_model_repo, commit_message=f\"Tracking info epoch {epoch+1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:38:31.990333Z","iopub.execute_input":"2025-05-15T09:38:31.990881Z","iopub.status.idle":"2025-05-15T09:38:32.149679Z","shell.execute_reply.started":"2025-05-15T09:38:31.990854Z","shell.execute_reply":"2025-05-15T09:38:32.149151Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## CPU + GPU Variant ","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom huggingface_hub import HfApi, login\nfrom peft import PeftModel\nfrom transformers import BlipForQuestionAnswering, BlipProcessor\nimport torch\nimport os\nimport pickle\n\nlogin(token=\"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\")\napi = HfApi()\n\ndef train(model, processor, train_dataloader, valid_dataloader, num_epochs, resume_training=False, patience=3):\n    best_model_repo = \"adityaav80/blip-basevqa-finetuned\"\n\n    # Use GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    start_epoch = 0\n    min_eval_loss = float('inf')\n    early_stopping_hook = 0\n\n    if resume_training:\n        latest_checkpoint_path = get_latest_checkpoint_from_hf(best_model_repo, \"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\")\n        if latest_checkpoint_path:\n            print(f\"Resuming from checkpoint at {latest_checkpoint_path}\")\n            base_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints/')\n            processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints/')\n            model = PeftModel.from_pretrained(base_model, f\"https://huggingface.co/{best_model_repo}/{latest_checkpoint_path}\")\n            model.to(device)\n\n            for param in model.parameters():\n                param.requires_grad = False\n            for name, param in model.named_parameters():\n                if \"lora\" in name:\n                    param.requires_grad = True\n            model.print_trainable_parameters()\n\n            start_epoch = int(latest_checkpoint_path.split('checkpoint')[-1])\n\n    scaler = torch.amp.GradScaler(device='cuda') if device.type == \"cuda\" else None\n    tracking_information = []\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        epoch_loss = 0\n\n        for batch in tqdm(train_dataloader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n            input_ids = batch.pop('input_ids').to(device)\n            pixel_values = batch.pop('pixel_values').to(device)\n            attention_mask = batch.pop('attention_mask').to(device)\n            labels = batch.pop('labels').to(device)\n\n            optimizer.zero_grad()\n\n            if device.type == \"cuda\":\n                with torch.amp.autocast(device_type='cuda'):\n                    outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n                                    attention_mask=attention_mask, labels=labels)\n                    loss = outputs.loss\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n                                attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                loss.backward()\n                optimizer.step()\n\n            epoch_loss += loss.item()\n\n        avg_epoch_loss = epoch_loss / len(train_dataloader)\n        tracking_information.append({'epoch': epoch + 1, 'train_loss': avg_epoch_loss})\n\n        # Evaluation\n        model.eval()\n        eval_loss = 0\n\n        with torch.no_grad():\n            for batch in tqdm(valid_dataloader, desc=f'Validation Epoch {epoch+1}/{num_epochs}'):\n                input_ids = batch.pop('input_ids').to(device)\n                pixel_values = batch.pop('pixel_values').to(device)\n                attention_mask = batch.pop('attention_mask').to(device)\n                labels = batch.pop('labels').to(device)\n\n                if device.type == \"cuda\":\n                    with torch.amp.autocast(device_type='cuda'):\n                        outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n                                        attention_mask=attention_mask, labels=labels)\n                        loss = outputs.loss\n                else:\n                    outputs = model(input_ids=input_ids, pixel_values=pixel_values,\n                                    attention_mask=attention_mask, labels=labels)\n                    loss = outputs.loss\n\n                eval_loss += loss.item()\n\n        avg_eval_loss = eval_loss / len(valid_dataloader)\n        tracking_information[-1]['eval_loss'] = avg_eval_loss\n        print(f\"Epoch {epoch+1}: Training Loss = {avg_epoch_loss}, Validation Loss = {avg_eval_loss}\")\n        scheduler.step()\n\n        checkpoint_repo_id = f\"adityaav80/blip-basevqa-finetuned-checkpoint-epoch{epoch+1}\"\n        model.push_to_hub(checkpoint_repo_id, commit_message=f\"Checkpoint at epoch {epoch+1}\")\n        print(f\"Checkpoint pushed to Hugging Face Hub: {checkpoint_repo_id}\")\n\n        if avg_eval_loss < min_eval_loss:\n            model.push_to_hub(best_model_repo, commit_message=f\"Epoch{epoch}\")\n            processor.push_to_hub(best_model_repo)\n            min_eval_loss = avg_eval_loss\n            early_stopping_hook = 0\n            print(f\"New best model pushed to Hugging Face Hub: {best_model_repo}\")\n        else:\n            early_stopping_hook += 1\n            if early_stopping_hook > patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    push_tracking_info_to_hub(tracking_information, best_model_repo, commit_message=f\"Tracking info epoch {epoch+1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:38:56.705038Z","iopub.execute_input":"2025-05-15T09:38:56.705334Z","iopub.status.idle":"2025-05-15T09:38:56.822143Z","shell.execute_reply.started":"2025-05-15T09:38:56.705309Z","shell.execute_reply":"2025-05-15T09:38:56.821478Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# from peft import PeftModel\n# NUM_EPOCHS = 20\n# PATIENCE = 3\n\n# # Start training\n# train(\n#     model=lora_model,\n#     processor=processor,\n#     train_dataloader=train_dataloader,\n#     valid_dataloader=valid_dataloader,\n#     num_epochs=NUM_EPOCHS,\n#     resume_training=True,\n#     patience=PATIENCE\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:45:29.673173Z","iopub.execute_input":"2025-05-15T09:45:29.673867Z","iopub.status.idle":"2025-05-15T09:45:29.677650Z","shell.execute_reply.started":"2025-05-15T09:45:29.673832Z","shell.execute_reply":"2025-05-15T09:45:29.676752Z"},"scrolled":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}